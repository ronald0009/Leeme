# [AWS Certified Cloud Practitioner](https://learning.oreilly.com/videos/aws-certified-cloud/9780135175507/9780135175507-accp_01_00_00_00)
## 1.1 Overview
- Ability to understand AWS as a solution provider
- What it is that AWS offers 
- Why we would choose Amazon Web Services
- Understand the cloud value proposition
  - Cost savings
  - Flexibility and agility
  - Time to market
- Knowledge of the AWS Global Infrastructure
  - Regions
  - Availability zones
  - Edge locations
- Architectural design principles
- Key services
- Understand security of the cloud and security in the cloud
  - Shared security model
  - Shared responsibility model
- Pricing, billing, and account management
## 1.2 Exam Preparation Resources
- The exam guide
- You have sample questions
- AWS blogs
- AWS documentation
## 1.3 Taking the Exam
- The first step in taking the exam is to register for the exam and you can do that on the [AWS training website](http://www.aws.training)
- Now the exam itself does have approximately 65 questions
- 90 minutes to complete those questions
## 1.4 Documentation and Study Material
- AWS website itself
- The FAQs
- AWS documentation
- AWS forums
- Quora
- StackOverflow
## 2.1 Introduction to Cloud Computing
- National Institute of Standards and Technology (NIST) Cloud Computing definition:
  - On-demand self service. And what that means is that we get what we need, when we need it.
  - Broad network access. Meaning that our services have access to a rich collection of networking tools, they have access to the internet, and we can reach them from the internet. 
  - Resource pooling speaks to the fact that a lot of the systems that we're talking about in cloud computing are shared systems. 
  - Rapid elasticity, we talk about the ability to scale out to meet demand, and scale in to save money.
- Service models
  - Infrastructure as a service.
  - Platforms as a service.
  - Software as a service.
- Deployment models
  - Private cloud, that is where a single organization invests their capital into their own data center to build the kinds of technologies that allow for the five points that we talked about: on-demand access, rapid elasticity, things like that. 
  - There's also a community cloud where perhaps several different organizations get together and pool their resources to build a cloud that is used by a small community of organizations. 
  - We also have the public cloud which is what Amazon Web Services is. 
  - A hybrid cloud means that we are leveraging something like Amazon Web Services for certain types of either data or computing needs, and also running certain types of computing and storage on premises. We have dedicated private fiber links between those two or VPN connections in order to share data between on premises and Amazon Web Services, a region within AWS.
## 2.2 Business Needs and Cloud Solutions
- We want resiliency in our infrastructure and our applications. That is to say that we want our applications, we want our infrastructure to mitigate issues as best as possible. 
- We want fault tolerance built in so that our applications can survive and tolerate faults as they come and go. We want our infrastructure to be self-healing to repair that fault automatically. 
- We want any type of an outage to recover as quickly as possible. 
- We also want security. We want to allow the right people in, keep the wrong people out. 
- We want tools to be able to monitor to be able to notify us if there's an issue, to help us respond to that issue. 
- We want tools that allow us to automatically change our rules to help prevent that from happening again. 
- We also want durability. We want our data to be durable to the point that we don't lose our data. 
- We want our applications to be performant. We want our end users to have a good experience, to get what they are after as quickly as possible. 
- We want to save money.
- We also want scalability, the ability to meet that demand when we have it, and then of course, the ability to scale back down, the ability to save money when we don't have that demand is also an important consideration and an important ability. 
- Lastly, we want as many of these things to be automated as possible. 
## 2.3 Overview of Amazon Web Services
- AWS, ultimately, is a very rich collection of services that solve just about any type of use case you can imagine. AWS gives us a collection of services dedicated to computing, such as virtual machines, serverless computing, containerized computing. We have services dedicated to block storage, object storage, and archival storage. We have services dedicated to application notifications such as just simple notifications, emails, queues, and so on. AWS also gives us a very rich collection of data stores from relational databases, to NoSQL databases, and memory caches, and more. We also have services dedicated to analytics allowing us to perform real-time analytics on streaming data to allow us to index and search vast data sets. We can also create and leverage services that allow us to perform ad hoc queries on petabyte scale or even exabyte-sized data sets. AWS also gives us a number of tools to allow us to create and manage a very secure network for our applications running in the cloud. We also have a number of tools that allow us to manage our deployment processes from code repos, to build servers, to code pipelines, and a number of other tools that can help us automate the creation and ongoing management of infrastructure within AWS. 
## 2.4 AWS Cloud Value Proposition
- AWS provides, On-Demand resources. That means to get what you need when you need it. 
- It gives rise to scalability, it gives rise to self-healing and so on, it gives rise to a level of resilience and a fault-tolerance and flexibility and agility. All of these good things are made possible by the On-Demand nature of AWS. 
- Amazon also gives us the pay as you go billing model. 
- Amazon also gives no long-term commitments, which means that we are free to throw things away. 
- And we also have within AWS a very highly automated ecosystem; the ability to repeat these things, right? So when you think about different types of infrastructure architectures that need to be repeated, perhaps you repeat an architecture in different environments; dev, test, QA, production. 
- And then of course AWS also provides a number of managed services, but managed services typically have inherent high availability, fault-tolerance, security, durability, and so on by leveraging managed services. 
- We can gain a level of flexibility and agility that was never possible with traditional on-premises IT. 
- We can gain an immediate scalability. I have heard first hand accounts from organizations who their on-premises data center was literally wall to wall, floor to ceiling out of room. They had absolutely, on both physical and virtual layers they had no ability to scale several applications, and so by expanding into AWS they gained an ability to scale that they never had before or at least that was not possible in their current situation with on-premises IT. 
- Like we've mentioned we have a lower time to market. All of theses things put together allow us to turn projects that may take months or longer with traditional IT into weeks. Imagine if you could measure your mean time between deployments in hours or minutes. 
- But all of these tools that we're going to talk about through this course, we will see just how we can, we will start to understand how we can lower that time to days or even hours. 
- We can also lower our variable costs, we can lower our upfront costs, and we benefit by easier cost allocation. With traditional on-premises IT, it becomes a challenge to know how our money is being spent by different applications.
- And then lastly, one potential benefit is that we can stop running data centers. 
## Module 2: introduction
### Learning objectives
#### 3.1 AWS Global Infrastructure
- Now let's review the AWS global infrastructure. 
- First, let's take a look at regions. Regions are the primary building block of infrastructure running in AWS. And, if we look at a world map here, we'll see starting from the west coast of the U.S. we have the Oregon region, California, GovCloud West. Moving towards the east coast we have Ohio, Virginia, we have Canada Central, we have Sao Paulo down in South America. Moving into Europe we have Ireland, London, Paris, Frankfurt, Germany. Moving into APAC we have Mumbai, Singapore, Sydney, Australia. We also have Ningxia, China, Beijing, China, we have Seoul, South Korea, and Tokyo, Japan. Regions are the primary location where we would choose to store our data or run our applications. 
- Now, a number of considerations would go into choosing a region. First of all we might consider the available services and features there. Keep in mind that not every region has every service, and there may be features of a particular service missing from certain regions. When Amazon releases new features or new services, they will often do so starting with one or two regions, allow that service or feature to exist in a preview mode before making it generally available to every region. So, if you are looking for a particular feature or service, be sure to check the AWS documentation and choose the region where that particular feature or service is available 
- We should also consider the cost of services. Different services, the cost of that service will vary according to regions. 
- And then, of course, we have disaster recovery. You may choose to run multiple regions for the sake of DR. We may need to consider security and compliance. Keep in mind that different countries have different laws regarding the sovereignty of data, and whether or not certain types of data can leave that region. And as a customer, it is our responsibility to be aware of those local laws, to be aware of the laws that govern us in our business and our data and to make choices, you know, that satisfy that. 
- Now within a particular region we have what we call availability zones. Here, looking at the U.S. west two region, the Oregon region, we have three availability zones. U.S. west two A, two B, and two C. These availability zones are analogous to collections of data centers. Each availability zone is at least one, sometimes maybe two or more data centers that are very, very close together, and each availability zone is spread out, you know, considering fault lines, flood planes, power grids, and so on. These availability zones are also connected with private fiber, allowing Amazon to achieve a very, very low latency with a very large degree of bandwidth between these availability zones. Amazon gives us this as a way to add a further degree of high availability and fall tolerance. 
- So keep that in mind, when you think of availability zones, think of having access to multiple data centers spread apart. When you think of high availability and fall tolerance we should also be thinking in terms of leveraging multiple availability zones. 
- Now let's talk about edge locations. Edge locations power several different things within AWS, namely the content delivery network and D and S. You will see that there is a significant global footprint of edge locations, and you will also notice that many of these reside outside of an AWS region. Now, it's important to understand that these edge locations power, again, content delivery services and D and S. We don't have direct access to the edge location like we do a region. We can generally choose large groups of edge locations, like North America, South America, Europe and so on, but we don't have any direct access to the edge location itself. The benefit of an edge location is that, in the case of content delivery, we can leverage the caching behavior of that edge location. So, let's say that we have an application running in Virginia, and we have an audience in Europe. Content delivery networks can help, in that instead of forcing our European audience to traverse the Atlantic ocean every time for every request, which would introduce latency, which would degrade their experience, we could leverage the content delivery network, leverage those edge locations, so that the content can be cached in edge locations closer to our audience. Again, so when we think about delivering content, improving end-user performance, when we think about leveraging content delivery networks, when we think about leveraging a globally distributed D and S system, that's what edge locations are for.
#### 3.2 Demo: AWS Management Console
- MFA pen. 
# Learning objectives
- Any exam will require knowledge of the core AWS services. For the certified cloud practitioner exam we will cover the basics of the core services without getting too deep into the technical details. We will review networking with virtual private cloud, Amazon EC2, durable block storage with Amazon EBS and object storage with Amazon S3.
## 4.1 Setting the Stage
- First we will start with an Amazon region. And of course we do that, we choose that region according to the features and services that are available, the cost of those features, our proximity to end users and so on. So we choose an Amazon region. And in that region we will leverage multiple availability zones. Again, when you think of multiple availability zones, we think of high availability and fault tolerance. And the converse is true, any time you hear the term, high availability or fault tolerance, we should also immediately think multiple availability zones. These multiple availability zones allow us to be resilient, not only to the loss of a device, but also to the loss of an entire data center. And so once we have chosen the region, we can then create a network within that region. A network that allows us to leverage all of those availability zones, and a network that provides isolation between tiers. Some parts of this network will be kept private, that is away from the internet, and other parts of this network will be public, that is, giving it access directly to and from the internet. We will place our application servers, in a private sub net. So preventing people on the internet coming to them directly We may put those behind a load balancer, and create security controls, that forces traffic to go through the load balancer, and prevents traffic from bypassing the load balancer. We may decide to run our own database. And we also need, for the data of that database, we want a durable storage, that is independent of the machine. So that if the machine fails, our storage is still there, our data survives the loss of the machine. We also want security controls that allow our application servers to reach our database, but prevent anything else from getting to the database directly. We may need object storage in order to store certain types of data, like log files, static content, CSS, Javascript, images and so on. We may then decide to serve dynamic content directly to the internet, directly to our users, from the load balancer. And then the browser may then point those pages to object storage, to retrieve things, static content like images, CSS, PDFs and so on. We would then need some DNS service like Route 53, to translate our domain name, into a usable IP address, for our load balancer or our object storage. And all of these things come together, there's a number of different tools and services that we're using here, and a key to understanding architecture is to understand when you would use a particular tool to solve a particular problem. 
## 4.2 Networking with Amazon VPC
- Now let's talk about networking with Amazon Virtual Private Cloud or VPC. Amazon VPC allows us to create logically isolated networks. So as a customer, you create your VPC, I create my VPC. Those are two completely different networks, and by default, they're completely isolated from the outside world. They cannot communicate with each other, they cannot communicate with the internet or any other network. And so right out of the box, they are completely isolated, and that isolation is what gives rise to network security. 
- They are created per account per region, and you know I've heard first hand accounts from enterprises who have tried to do everything. All of their applications and whatnot, within a single VPC, and it's just gets really challenging really quickly. So I always encourage folks to really embrace the idea of multiple VPCs. Because one, if you're leveraging multiple accounts, as we will talk about later, and/or leveraging multiple regions you will have multiple VPCs anyway, right? So there are a number of reasons why you would divide your applications and infrastructures into multiple networks, and we will talk about that later on. One VPC does span a single region and within that region, one VPC can use all of the availability zones. So we've talked about how availability zones give rise to high-availability and fault-tolerance, allowing us to be resilient to the loss of a data center. And so, for applications running on EC2, the architecture begins with the network. In order to achieve high-availability, fault tolerance by leveraging multiple AZs, we have to design a network that makes use of those AZs. 
- Different VPCs can peer with one another, and there are a number of reasons why you might want to do that. Again, because a VPC is isolated from other networks, by putting, perhaps a web application in one VPC and your data stores in another VPC, and peering them together allowing them to communicate, you gain that isolation. You add further protections to your databases, in that way. That's just one example of why we might separate and peer VPCs together. 
- VPCs also allow us to create internet and VPN gateways. There are mechanisms that allow our VPCs to communicate with the internet, to communicate over VPNs, back to on premises. And we have a number of security mechanisms, routing controls, so that we can control the flow of traffic. Say that some part of our network has allowed to communicate with the internet, some part of our network has allowed to communicate to our on premises network via VPN. And we have several different types of firewalls that we can use to filter the traffic based on protocols, ports, sources, and destinations. 
- Now in this example, we might take a look at the US-West-2-Oregon region. And in that region, we might choose to create a VPC. And you will see here, at in doing so, we choose a particular range of IP addresses. We say that this particular VPC will use a range of IP addresses starting with 10.2 and a slash 16, would give us a little over 65,000 IP addresses. Now we could create multiple VPCs in that region. Keep in mind that there are soft limits. The initial soft limit for VPC creation is five VPCs per region. And of course you can lift that limit by submitting a ticket to Amazon support. Now, you'll also notice in this example is these two VPCs, even though one has a slightly different range, they overlap. The IP ranges of these two VCPs overlap by several thousand IP addresses, and that's okay, we're allowed to do that. What we couldn't do in this example is peer these two VPCs, but keep in mind, that it is possible to have two VPCs with different IP ranges or the same IP ranges. 
- Now, it's not enough just to create a VPC. We also have to divide that VPC into subnets. When we create subnets, subnets are tied specifically one to one with availability zones. So when we launch a virtual machine, we don't launch the virtual machine directly into a VPC. We launch it into a subnet. And by launching it into that subnet, that's how we determine, not only what network it belongs to, but what availability zone that machine will reside in. And so, in this example, we're going to divide this VPC into multiple subnets one per availability zone. Each one with a completely different IP range. Within a VPC, each subnet has to have it's own unique range of IP addresses. 
- And remember, this an important point to remember that subnets are specific to a single availability zone. Alright, so now let's take a look at a three-tier architecture, a classic example of a three-tier design. So we might have a public load balancing tier, a private application tier, and a private database tier. And from a networking perspective, we would want them to gain access to the internet. We would also want to achieve isolation and security between those tiers, and we want to enable high-availability by leveraging multiple availability zones. And of course, we also want fault-tolerance. Now the way that we would achieve that, here again, we would create a VPC in a region, leveraging all of the availability zones. And we would create a subnet per tier. So we have a subnet specifically designed for our load balancer, a subnet specific for our application servers, and a subnet specifically for our database. That gives us isolation. Because, the security controls that we would apply to these subnets, the firewalls that we'd apply, require different ports. The load balancer, our application and our database will all require different ports. And so by putting them into different subnets, we gain the ability to tailor the security mechanisms for the ports that are being used. Right, keep that in mind, that our subnets in this regard are providing us isolation, and then for the sake of high-availability, we duplicate that across the various availability zones. 
- We then, can put or load balancer as a managed device a managed service, the load balancer can then span. all of the availability zones within the tier, within that subnet set aside for that tier. We can then place our application servers into their own private tier, and our databases in their tiers. And then, of course, if a machine fails, we still have another two machines. If an entire availability zone fails, if we lose an entire data center, we still have another two. That is high-availability, that is fault tolerance. 
- Right, so as we've seen there, we can see that subnets enable security via isolation. That the different subnets enable us to tailor security mechanisms specifically to the purpose for that subnet, for that tier. Because subnets allow us to leverage multiple availability zones, they then enable high-availability, and fault-tolerance. 
  - They also can enable performance.
